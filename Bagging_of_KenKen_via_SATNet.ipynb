{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ilangofman/SATNet-Logic-Puzzles/blob/main/Bagging_of_KenKen_via_SATNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KenKen Solving With SATNet Using Bagging Ensemble Learning"
      ],
      "metadata": {
        "id": "1MrMBbuoRDx5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the model artifacts for the original SATNet model \n",
        "\n",
        "Po-Wei Wang, Priya L. Donti, Bryan Wilder, and J. Zico Kolter. Satnet: Bridging deep learning and logical reasoning\n",
        "using a differentiable satisfiability solver. In Proceedings of ICML’19, pages 6545–6554, 2019. URL https: [github.com/locuslab/SATNet.git.](https://github.com/locuslab/SATNet.git)"
      ],
      "metadata": {
        "id": "PaaTiSBSRHjh"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRDr098xDddK",
        "outputId": "eb3f916e-eb14-43c2-e6be-c1643856c6bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone https://github.com/locuslab/SATNet\n",
        "%cd SATNet\n",
        "!python setup.py develop > install.log 2>&1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SATNet'...\n",
            "remote: Enumerating objects: 101, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 101 (delta 0), reused 2 (delta 0), pack-reused 96\u001b[K\n",
            "Receiving objects: 100% (101/101), 497.29 KiB | 4.48 MiB/s, done.\n",
            "Resolving deltas: 100% (37/37), done.\n",
            "/content/SATNet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7a4bS78DqWF",
        "outputId": "f8a9d02e-1079-4258-80f8-76429ca6cf16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "import argparse\n",
        "from collections import namedtuple\n",
        "\n",
        "import numpy as np\n",
        "import numpy.random as npr\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, Markdown, Latex, clear_output\n",
        "import tqdm\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import random\n",
        "random.seed(6)\n",
        "\n",
        "\n",
        "\n",
        "if not torch.cuda.is_available(): \n",
        "    print('[WARNING] Not using GPU.')\n",
        "    print('Please select \"Runtime -> Change runtime type\" and switch to GPU for better performance')\n",
        "else:\n",
        "    print('Using', torch.cuda.get_device_name(0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BzTg_RFDddO"
      },
      "source": [
        "# Introduction to SATNet\n",
        "SATNet is a differentiable (smoothed) maximum satisfiability (MAXSAT) solver that can be integrated into the loop of larger deep learning systems.  Our (approximate) solver is based upon a fast coordinate descent approach to solving the semidefinite program (SDP) associated with the MAXSAT problem.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6wQKRvsDddO",
        "outputId": "40136b62-eb92-4d18-967a-4c13fb1175d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import satnet\n",
        "print('SATNet document\\n', satnet.SATNet.__doc__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SATNet document\n",
            " Apply a SATNet layer to complete the input probabilities.\n",
            "\n",
            "    Args:\n",
            "        n: Number of input variables.\n",
            "        m: Rank of the clause matrix.\n",
            "        aux: Number of auxiliary variables.\n",
            "\n",
            "        max_iter: Maximum number of iterations for solving\n",
            "            the inner optimization problem.\n",
            "            Default: 40\n",
            "        eps: The stopping threshold for the inner optimizaiton problem.\n",
            "            The inner Mixing method will stop when the function decrease\n",
            "            is less then eps times the initial function decrease.\n",
            "            Default: 1e-4\n",
            "        prox_lam: The diagonal increment in the backward linear system\n",
            "            to make the backward pass more stable.\n",
            "            Default: 1e-2\n",
            "        weight_normalize: Set true to perform normlization for init weights.\n",
            "            Default: True\n",
            "\n",
            "    Inputs: (z, is_input)\n",
            "        **z** of shape `(batch, n)`: \n",
            "            Float tensor containing the probabilities (must be in [0,1]).\n",
            "        **is_input** of shape `(batch, n)`: \n",
            "            Int tensor indicating which **z** is a input.\n",
            "\n",
            "    Outputs: z\n",
            "        **z** of shape `(batch, n)`:\n",
            "            The prediction probabiolities.\n",
            "\n",
            "    Attributes: S\n",
            "        **S** of shape `(n, m)`:\n",
            "            The learnable clauses matrix containing `m` clauses \n",
            "            for the `n` variables.\n",
            "\n",
            "    Examples:\n",
            "        >>> sat = satnet.SATNet(3, 4, aux=5)\n",
            "        >>> z = torch.randn(2, 3)\n",
            "        >>> is_input = torch.IntTensor([[1, 1, 0], [1,0,1]])\n",
            "        >>> pred = sat(z, is_input)\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0ikehENrFVz"
      },
      "source": [
        "# Building SATNet-based Models\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bk11WBMPDddS"
      },
      "source": [
        "class KenKenSolver(nn.Module):\n",
        "    def __init__(self, input_variables, aux, m):\n",
        "        super(KenKenSolver, self).__init__()\n",
        "        n = input_variables\n",
        "        self.sat = satnet.SATNet(n, m, aux)\n",
        "\n",
        "    def forward(self, y_in, mask):\n",
        "        out = self.sat(y_in, mask)\n",
        "        del y_in, mask\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4xZ5riRrSkh"
      },
      "source": [
        "The experimental parameters we use in the paper are below."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KenKen Dataset"
      ],
      "metadata": {
        "id": "ut26blwIdXBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "# Opening JSON file\n",
        "f = open('/content/games_json-3x3_cage2.json')\n",
        "  \n",
        "# returns JSON object as \n",
        "# a dictionary\n",
        "data = json.load(f)"
      ],
      "metadata": {
        "id": "bGstXVY4dU6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(data)"
      ],
      "metadata": {
        "id": "zViJ4h_hfSRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Dataset and create encoding \n",
        "\n",
        "For different configurations of the game, some constants will need to be changed. Such as:\n",
        "\n",
        "- *GAME_SIZE* : the value of n in an *n x n* board. \n",
        "- *max_cage_total* The maximum total of a cage\n",
        "- *MAX_CELL_SIZE* : the maximum number of cells in a cage\n",
        "- *MASK_OUT_SINGLE*: boolean flag to mask out single cells from learning. "
      ],
      "metadata": {
        "id": "HEjC0o32fmHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_dataset = []\n",
        "solutions = []\n",
        "kenken_masks = []\n",
        "\n",
        "GAME_SIZE = 3\n",
        "MAX_CELL_SIZE = 2\n",
        "\n",
        "num_possible_cages = GAME_SIZE ** 2 \n",
        "\n",
        "max_cage_total = 5 \n",
        "\n",
        "MASK_OUT_SINGLE = True\n",
        "\n",
        "for game in data:\n",
        "  # For each cell in the game, add a boolean variable to represent all possible values it can take\n",
        "  # The first GAME_SIZE multiplication ensures one variable for all possible variables in one cell.\n",
        "  # multiplying again by GAME_SIZE squared, adds variables for all cells in the game \n",
        "  # empty_value_variables = [0] * (GAME_SIZE * (GAME_SIZE ** 2))\n",
        "  solution_value_variables = []\n",
        "  empty_value_variables = []\n",
        "  input_mask = []\n",
        "\n",
        "\n",
        "  for i in range(len(game['solution'])):\n",
        "    for j in range(len(game['solution'][0])):\n",
        "      soln_value_variable = [0] * (GAME_SIZE)\n",
        "      soln_value_variable[game['solution'][i][j] - 1] = 1\n",
        "\n",
        "      solution_value_variables += soln_value_variable\n",
        "\n",
        "      cell_group = game['cellGroups'][i][j]\n",
        "      operation_for_current_cell = game['groupValues'][cell_group-1][1]\n",
        "\n",
        "      value_variables = [0] * GAME_SIZE \n",
        "      if not MASK_OUT_SINGLE or operation_for_current_cell[0] == \"+\" or operation_for_current_cell[0] == \"-\" or operation_for_current_cell[0] == \"*\" or operation_for_current_cell[0] == \"/\":\n",
        "        empty_value_variables += value_variables\n",
        "        input_mask += [0] * GAME_SIZE\n",
        "      else:\n",
        "        empty_value_variables += soln_value_variable\n",
        "        input_mask += [1] * GAME_SIZE\n",
        "\n",
        "\n",
        "  CAGE_VARIABLES = []\n",
        "  CAGE_TOTALS = []\n",
        "  # Loop through the board and create cage membership variables\n",
        "  for i in range(len(game['cellGroups'])):\n",
        "    for j in range(len(game['cellGroups'][0])):\n",
        "      cage_booleans = [0] * num_possible_cages\n",
        "      cage_booleans[game['cellGroups'][i][j] - 1] = 1\n",
        "\n",
        "      CAGE_VARIABLES += cage_booleans\n",
        "\n",
        "  # Create the total values for the cages\n",
        "  for i in range(len(game['groupValues'])):\n",
        "    possible_cage_totals = [0] * max_cage_total\n",
        "    current_group_total = int(game['groupValues'][i][1])\n",
        "\n",
        "    possible_cage_totals[current_group_total-1] = 1\n",
        "\n",
        "    CAGE_TOTALS += possible_cage_totals\n",
        "\n",
        "  # There is a high change the number of cages generated in the game is going to be less than \n",
        "  # the number of max possible cages. Just so we are consistent, lets add the remainding cage totals padding of 0s\n",
        "\n",
        "  for i in range(num_possible_cages - len(game['groupValues'])):\n",
        "    possible_cage_totals = [0] * max_cage_total\n",
        "    CAGE_TOTALS += possible_cage_totals\n",
        "\n",
        "\n",
        "  # input_mask = [0] * len(empty_value_variables) \n",
        "  input_mask += [1] * (len(CAGE_VARIABLES) + len(CAGE_TOTALS))\n",
        "\n",
        "  print(\"Number of empty value variables = \", len(empty_value_variables))\n",
        "  print(\"Number of solution value variables = \", len(solution_value_variables))\n",
        "  print(\"Number of cage membership variables = \", len(CAGE_VARIABLES))\n",
        "  print(\"Number of cage total= \", len(CAGE_TOTALS))\n",
        "  print(\"Input mask len\", len(input_mask))\n",
        "  print(input_mask)\n",
        "\n",
        "  X_dataset.append(empty_value_variables + CAGE_VARIABLES + CAGE_TOTALS)\n",
        "  solutions.append(solution_value_variables + CAGE_VARIABLES + CAGE_TOTALS)\n",
        "  kenken_masks.append(input_mask)\n",
        "\n"
      ],
      "metadata": {
        "id": "DD-ghB0mahw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_dataset = torch.tensor(X_dataset, dtype=torch.float)\n",
        "kenken_masks = torch.tensor(kenken_masks, dtype=torch.int32)\n",
        "solutions = torch.tensor(solutions, dtype=torch.float)"
      ],
      "metadata": {
        "id": "Gj_-MAYakIMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Hyperparameters"
      ],
      "metadata": {
        "id": "x5tmlXseR-6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from exps.sudoku import FigLogger, find_unperm\n",
        "args_dict = {'lr': 2e-4 * 5, \n",
        "             'cuda': torch.cuda.is_available(), \n",
        "             'batchSz': 40,\n",
        "             'mnistBatchSz': 50,\n",
        "             'boardSz': GAME_SIZE, \n",
        "             'm': 600,\n",
        "             'aux': 600,\n",
        "             'nEpoch': 50\n",
        "            }\n",
        "args = namedtuple('Args', args_dict.keys())(*args_dict.values())"
      ],
      "metadata": {
        "id": "HiLNzoDnwM70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if args.cuda: X_dataset, kenken_masks,solutions = X_dataset.cuda(), kenken_masks.cuda(), solutions.cuda()\n"
      ],
      "metadata": {
        "id": "FxVzQpwkwQSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split data into train and test "
      ],
      "metadata": {
        "id": "YSeScOqLSCc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N = X_dataset.size(0)\n",
        "nTrain = int(N*0.9)\n"
      ],
      "metadata": {
        "id": "xsn849BMjTF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of Models in the bag\n",
        "NUM_BAG_MODELS = 10\n",
        "SUBSAMPLE_PERCENT = 0.8\n",
        "\n",
        "\n",
        "\n",
        "train_datasets = []\n",
        "\n",
        "for i in range(NUM_BAG_MODELS):\n",
        "  indecies = random.sample(range(nTrain), int(nTrain*SUBSAMPLE_PERCENT))\n",
        "  train_datasets.append(TensorDataset(X_dataset[indecies], kenken_masks[indecies], solutions[indecies]))\n"
      ],
      "metadata": {
        "id": "ZQUO7uDrpY1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "kenken_test =  TensorDataset(X_dataset[nTrain:], kenken_masks[nTrain:], solutions[nTrain:])"
      ],
      "metadata": {
        "id": "UGZ-zsslab8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jca3yOrFxxTO"
      },
      "source": [
        "## KenKen Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run(boardSz, epoch, model, optimizer, logger, dataset, batchSz, to_train=False, unperm=None):\n",
        "\n",
        "    loss_final, err_final = 0, 0\n",
        "\n",
        "    loader = DataLoader(dataset, batch_size=batchSz)\n",
        "    tloader = tqdm(enumerate(loader), total=len(loader))\n",
        "\n",
        "    print(\"loader len: \", len(loader))\n",
        "\n",
        "    for i,(data,is_input,label) in tloader:\n",
        "        if to_train: optimizer.zero_grad()\n",
        "        preds = model(data.contiguous(), is_input.contiguous())\n",
        "        # print(\"PRED::: \", preds)\n",
        "        # print(\"label::: \", label)\n",
        "        \n",
        "        loss = nn.functional.binary_cross_entropy(preds, label)\n",
        "\n",
        "        if to_train:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        err = computeErr(preds.data, label, boardSz, unperm)\n",
        "        # err = computeErr(preds.data, label, boardSz, unperm)/batchSz\n",
        "        tloader.set_description('Epoch {} {} Loss {:.4f} Err: {:.4f}'.format(epoch, ('Train' if to_train else 'Test '), loss.item(), err))\n",
        "        loss_final += loss.item()\n",
        "        err_final += err\n",
        "\n",
        "    loss_final, err_final = loss_final/len(loader), err_final/len(loader)\n",
        "    logger.log((epoch, loss_final, err_final))\n",
        "\n",
        "    if not to_train:\n",
        "        print('TESTING SET RESULTS: Average loss: {:.4f} Err: {:.4f}'.format(loss_final, err_final))\n",
        "\n",
        "    #print('memory: {:.2f} MB, cached: {:.2f} MB'.format(torch.cuda.memory_allocated()/2.**20, torch.cuda.memory_cached()/2.**20))\n",
        "    torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "lvldT06NpBiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(args, epoch, model, optimizer, logger, dataset, batchSz, unperm=None):\n",
        "    run(args, epoch, model, optimizer, logger, dataset, batchSz, True, unperm)\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(args, epoch, model, optimizer, logger, dataset, batchSz, unperm=None):\n",
        "    run(args, epoch, model, optimizer, logger, dataset, batchSz, False, unperm)\n",
        "\n",
        "@torch.no_grad()\n",
        "def computeErr(pred_flat, label,  n, unperm):\n",
        "\n",
        "    res = pred_flat[:, : GAME_SIZE * (GAME_SIZE ** 2)]\n",
        "    res = res.view(-1, GAME_SIZE, GAME_SIZE, GAME_SIZE)\n",
        "    res_values = torch.argmax(res, dim=3) + 1\n",
        "\n",
        "    sol = label[:, : GAME_SIZE * (GAME_SIZE ** 2)]\n",
        "    sol = sol.view(-1, GAME_SIZE, GAME_SIZE, GAME_SIZE)\n",
        "    solution_values = torch.argmax(sol, dim=3) + 1\n",
        "\n",
        "    res_values = res_values.view(-1, GAME_SIZE *GAME_SIZE)\n",
        "    solution_values = solution_values.view(-1, GAME_SIZE *GAME_SIZE)\n",
        "\n",
        "\n",
        "    num_correct = ((abs(res_values - solution_values)).sum(dim=1) == 0).sum()\n",
        "\n",
        "\n",
        "    total = len(label)\n",
        "\n",
        "    print(\"ERROR RATE\",float((total - num_correct) / total) )\n",
        "\n",
        "    return float((total - num_correct) / total)"
      ],
      "metadata": {
        "id": "AZIej5loo6y1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the model Ensemble"
      ],
      "metadata": {
        "id": "EL4adFQHSIBA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the ensemble"
      ],
      "metadata": {
        "id": "8bCeTpCuatL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = []\n",
        "for i in range(NUM_BAG_MODELS):\n",
        "  kenken_model = KenKenSolver(len(X_dataset[0]), args.aux, args.m)\n",
        "  models.append(kenken_model)"
      ],
      "metadata": {
        "id": "P-BdeV9Gajj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uMVod2ZDddf"
      },
      "source": [
        "for i in range(NUM_BAG_MODELS):\n",
        "  print(\"Training Model -->\", i+1 )\n",
        "\n",
        "  if args.cuda: models[i] = models[i].cuda()\n",
        "\n",
        "  plt.ioff()\n",
        "  optimizer = optim.Adam(models[i].parameters(), lr=args.lr)\n",
        "\n",
        "  fig, axes = plt.subplots(1,2, figsize=(10,4))\n",
        "  plt.subplots_adjust(wspace=0.4)\n",
        "  train_logger = FigLogger(fig, axes[0], 'Traininig')\n",
        "  test_logger = FigLogger(fig, axes[1], 'Testing')\n",
        "\n",
        "  test(args.boardSz, 0, models[i], optimizer, test_logger, kenken_test, args.batchSz)\n",
        "  plt.pause(0.01)\n",
        "\n",
        "  for epoch in range(1, args.nEpoch+1):\n",
        "      train(args.boardSz, epoch, models[i], optimizer, train_logger, train_datasets[i], args.batchSz)\n",
        "      test(args.boardSz, epoch, models[i], optimizer, test_logger, kenken_test, args.batchSz)\n",
        "      display(fig)\n",
        "\n",
        "  torch.save(models[i].state_dict(), '/content/drive/MyDrive/2547/model_4x4_cell_size_3_m_' + str(args.m) + \"_aux_\" + str(args.aux) +  '_bag_' + str(i) + '.pt')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate Ensemble"
      ],
      "metadata": {
        "id": "dd0JhJUXbWz1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the saved  models"
      ],
      "metadata": {
        "id": "6sF01JfASOn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble = []\n",
        "\n",
        "for i in range(10):\n",
        "  model = KenKenSolver(len(X_dataset[0]), 600, 600)\n",
        "  model.load_state_dict(torch.load('/content/drive/MyDrive/2547/model_4x4_cell_size_3_m_600_aux_600_bag_' + str(i) + '.pt'))\n",
        "  ensemble.append(model)"
      ],
      "metadata": {
        "id": "XbcamC1ia9fj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each prediction, convert the encoded values to the board numeric values"
      ],
      "metadata": {
        "id": "KVHMjLHPvYSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def get_prediction_board(pred_flat, label,  n):\n",
        "\n",
        "    res = pred_flat[:, : GAME_SIZE * (GAME_SIZE ** 2)]\n",
        "    res = res.view(-1, GAME_SIZE, GAME_SIZE, GAME_SIZE)\n",
        "    res_values = torch.argmax(res, dim=3) + 1\n",
        "\n",
        "    sol = label[:, : GAME_SIZE * (GAME_SIZE ** 2)]\n",
        "    sol = sol.view(-1, GAME_SIZE, GAME_SIZE, GAME_SIZE)\n",
        "    solution_values = torch.argmax(sol, dim=3) + 1\n",
        "\n",
        "    res_values = res_values.view(-1, GAME_SIZE *GAME_SIZE)\n",
        "    solution_values = solution_values.view(-1, GAME_SIZE *GAME_SIZE)\n",
        "\n",
        "\n",
        "    num_correct = ((abs(res_values - solution_values)).sum(dim=1) == 0).sum()\n",
        "\n",
        "    return res_values, solution_values"
      ],
      "metadata": {
        "id": "MrypT4G-R-Ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get all the predictions for the entire ensemble "
      ],
      "metadata": {
        "id": "gLp0lE2svgrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from zmq import device\n",
        "\n",
        "total_predictions = [[] for i in range(len(ensemble))]\n",
        "correct_labels = []\n",
        "\n",
        "loader = DataLoader(kenken_test, batch_size=args.batchSz)\n",
        "tloader = tqdm(enumerate(loader), total=len(loader))\n",
        "\n",
        "for i,(data,is_input,label) in tloader:\n",
        "  for j, model in enumerate(ensemble):\n",
        "    if args.cuda: model = model.cuda()\n",
        "    model.eval()\n",
        "    preds = model(data.contiguous(), is_input.contiguous())\n",
        "    res, solutions = get_prediction_board(preds, label, args.boardSz)\n",
        "    total_predictions[j] += res.tolist()\n",
        "  correct_labels += solutions.tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "64c92c0316c542e090e3fd2f1c2e371a",
            "0f97ce1cb5e146c5853482ed289f1c18",
            "3eba4d5153194a7f98407896b18387f4",
            "9dd302350e8542efa06c5c5e499e98a8",
            "a19aab7e08744b5397d77f59adc90ba5",
            "359ef740d59a4ae6a01246dea5c0d1c7",
            "b13c6b2e002c4bbd82ab658aadcf920b",
            "3b9e837ac1b54bf2a76686eafe7ce607",
            "57ee55c5991a4c6ea4f89dc5211e2ba5",
            "12ba6dc5d1794c0d8050398e54c03cee",
            "eb0c88611ed04380b3508be02f03cc1b"
          ]
        },
        "id": "fUrTi15uSRBc",
        "outputId": "661ab978-0f38-4ebd-c49b-59aaeaf67aa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/7 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "64c92c0316c542e090e3fd2f1c2e371a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0 \n",
        "correct_per = [0 for _ in range(len(ensemble))]\n",
        "for i in range(len(correct_labels)):\n",
        "  c = 0\n",
        "  for j in range(len(ensemble)):\n",
        "    if total_predictions[j][i] == correct_labels[i]:\n",
        "      c = 1\n",
        "      correct_per[j] += 1 \n",
        "  correct += c\n",
        "print(\"Total Correct: \", correct)\n",
        "print(\"Accuracy: \", correct/len(correct_labels))\n",
        "print(\"Correct each: \", correct_per)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3Qz_S9-TcqO",
        "outputId": "cf3a4552-6ad7-4416-dd7d-3a49852f9331"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Correct:  72\n",
            "Accuracy:  0.28346456692913385\n",
            "Correct each:  [27, 37, 25, 33, 33, 33, 27, 27, 34, 27]\n"
          ]
        }
      ]
    }
  ]
}